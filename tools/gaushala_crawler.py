"""
Gaushala Web Crawler
--------------------
Crawls helpgaushala.com and other sources for Gaushala data.
Generates JSON and JS data files for the SanatanOS Gau Seva app.

Usage:
    python gaushala_crawler.py --source helpgaushala
    python gaushala_crawler.py --all

Requirements:
    pip install requests beautifulsoup4 lxml geopy
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import re
import sys
import argparse
from typing import List, Dict, Optional

# Headers to mimic browser
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5',
}

# State codes mapping
STATE_CODES = {
    'andhra pradesh': 'AP', 'arunachal pradesh': 'AR', 'assam': 'AS',
    'bihar': 'BR', 'chhattisgarh': 'CG', 'goa': 'GA', 'gujarat': 'GJ',
    'haryana': 'HR', 'himachal pradesh': 'HP', 'jharkhand': 'JH',
    'karnataka': 'KA', 'kerala': 'KL', 'madhya pradesh': 'MP',
    'maharashtra': 'MH', 'manipur': 'MN', 'meghalaya': 'ML',
    'mizoram': 'MZ', 'nagaland': 'NL', 'odisha': 'OR', 'punjab': 'PB',
    'rajasthan': 'RJ', 'sikkim': 'SK', 'tamil nadu': 'TN',
    'telangana': 'TG', 'tripura': 'TR', 'uttar pradesh': 'UP',
    'uttarakhand': 'UK', 'west bengal': 'WB', 'delhi': 'DL',
}

def get_state_code(state: str) -> str:
    """Get ISO state code"""
    return STATE_CODES.get(state.lower().strip(), state[:2].upper())

def clean_phone(phone: str) -> Optional[str]:
    """Clean and format phone number"""
    if not phone: return None
    cleaned = re.sub(r'[^\d+]', '', phone)
    if cleaned.startswith('+'): cleaned = cleaned[1:]
    if len(cleaned) == 10: cleaned = '91' + cleaned
    return cleaned if len(cleaned) >= 10 else None

# Geocoding support
try:
    from geopy.geocoders import Nominatim
    from geopy.exc import GeocoderTimedOut, GeocoderServiceError
    GEOPY_AVAILABLE = True
except ImportError:
    GEOPY_AVAILABLE = False


def geocode_address(address: str, geolocator) -> Optional[Dict]:
    """Geocode address to lat/lng"""
    if not GEOPY_AVAILABLE or not geolocator:
        return None
    try:
        # Append India for better accuracy
        location = geolocator.geocode(f"{address}, India", timeout=10)
        if location:
            return {
                "lat": round(location.latitude, 4),
                "lng": round(location.longitude, 4)
            }
    except Exception as e:
        print(f"    [!] Geocoding error: {e}")
    return None


class GaushalaCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update(HEADERS)
        self.results = []
        
        # Initialize Geocoder
        self.geolocator = None
        if GEOPY_AVAILABLE:
            self.geolocator = Nominatim(user_agent="gaushala_crawler_bot_india")
            print("[OK] Geocoder initialized (Nominatim)")
        else:
            print("[!] geopy not installed. Geocoding disabled. (pip install geopy)")
    
    def save_results(self, filename: str):
        """Save to JSON and update App JS"""
        timestamp = time.strftime('%Y-%m-%d')
        
        # 1. Save JSON (Backup/Analysis)
        output = {
            'lastUpdated': timestamp,
            'count': len(self.results),
            'gaushalas': self.results
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(output, f, indent=2, ensure_ascii=False)
        print(f"\n[OK] Saved JSON payload to {filename}")
        
        # 2. Update App Data (crawled-data.js)
        js_path = 'apps/gau-seva/js/crawled-data.js'
        try:
            with open(js_path, 'w', encoding='utf-8') as f:
                f.write(f"// Auto-generated by gaushala_crawler.py on {timestamp}\n")
                f.write("window.CRAWLED_GAUSHALAS = ")
                json.dump(self.results, f, indent=4, ensure_ascii=False)
                f.write(";\n")
            print(f"[OK] Updated App Data: {js_path}")
        except Exception as e:
            print(f"[!] Could not update App JS (check path): {e}")

    def crawl_helpgaushala(self, max_pages: int = 5) -> List[Dict]:
        """Crawl helpgaushala.com"""
        print("\n[>] Crawling helpgaushala.com...")
        base_url = "https://helpgaushala.com/gaushala-list/"
        
        count = 0
        for page in range(1, max_pages + 1):
            url = f"{base_url}page/{page}/" if page > 1 else base_url
            print(f"  [>] Scanning Page {page}...")
            
            try:
                response = self.session.get(url, timeout=15)
                if response.status_code != 200:
                    print(f"  [!] Page {page} returned {response.status_code}")
                    break
                
                soup = BeautifulSoup(response.text, 'lxml')
                cards = soup.select('.gaushala-card, .listing-item, article')
                
                if not cards: break
                
                for card in cards:
                    try:
                        data = self._parse_helpgaushala_card(card)
                        if data:
                            self.results.append(data)
                            count += 1
                            if self.geolocator: time.sleep(1.1) 
                    except Exception as e:
                        print(f"    [!] Error: {e}")
                        
            except Exception as e:
                print(f"  [!] Failed: {e}")
                break
                
        print(f"  [OK] Extracted {count} Gaushalas")
        return self.results

    def _parse_helpgaushala_card(self, card) -> Optional[Dict]:
        name_el = card.select_one('h2, h3, .title, .name')
        if not name_el: return None
        
        name = name_el.get_text(strip=True)
        
        loc_el = card.select_one('.location, .address, p')
        raw_address = loc_el.get_text(strip=True) if loc_el else ""
        
        pin_match = re.search(r'\b\d{6}\b', raw_address)
        pincode = pin_match.group(0) if pin_match else None
        
        phone_el = card.select_one('a[href^="tel:"], .phone')
        phone = None
        if phone_el:
            phone_text = phone_el.get('href', '').replace('tel:', '') or phone_el.get_text()
            phone = clean_phone(phone_text)
            
        whatsapp_el = card.select_one('a[href*="wa.me"]')
        whatsapp = None
        if whatsapp_el:
            wa_match = re.search(r'(\d{10,12})', whatsapp_el['href'])
            if wa_match: whatsapp = clean_phone(wa_match.group(1))
        
        if not whatsapp and phone and phone.startswith('91') and len(phone) == 12:
            whatsapp = phone
            
        state, city, district = self._parse_location(raw_address)
        
        geo = None
        if self.geolocator and raw_address:
            geo = geocode_address(f"{city}, {state}", self.geolocator)
        
        return {
            'id': f"HG-{get_state_code(state)}-{len(self.results)+1:04d}",
            'name': name,
            'state': state,
            'city': city,
            'district': district,
            'pincode': pincode,
            'address': raw_address,
            'phone': phone,
            'whatsapp': whatsapp,
            'type': 'Trust',
            'verified': bool(phone),
            'geo': geo,
            'source': 'helpgaushala.com',
            'last_crawled': time.strftime('%Y-%m-%d')
        }
    
    def _parse_location(self, text: str) -> tuple:
        parts = [p.strip() for p in text.split(',')]
        state = 'Unknown'
        for p in parts:
            clean_p = p.lower().strip()
            if clean_p in STATE_CODES:
                state = STATE_CODES[clean_p].title() if len(clean_p) > 2 else clean_p.upper()
                for k, v in STATE_CODES.items():
                    if k == clean_p: state = k.title(); break
                break
        
        city = parts[0] if parts else "Unknown"
        district = parts[1] if len(parts) > 1 else city
        return (state, city, district)

    def crawl_rajasthan_gopalan(self):
        print("\n[>] Crawling Rajasthan (Stub)...")
    def crawl_maharashtra_goseva(self):
        print("\n[>] Crawling Maharashtra (Stub)...")
    def crawl_awbi(self):
        print("\n[>] Crawling AWBI (Stub)...")


def main():
    parser = argparse.ArgumentParser(description='Gaushala Web Crawler')
    parser.add_argument('--source', type=str, default='helpgaushala',
                        help='Source to crawl: helpgaushala, rajasthan, etc.')
    parser.add_argument('--pages', type=int, default=5,
                        help='Max pages to crawl for helpgaushala')
    
    args = parser.parse_args()
    
    crawler = GaushalaCrawler()
    
    source = args.source.lower()
    
    if source == 'helpgaushala':
        crawler.crawl_helpgaushala(max_pages=args.pages)
    elif source == 'all':
        crawler.crawl_helpgaushala(max_pages=args.pages)
        crawler.crawl_rajasthan_gopalan()
        # Add others
    else:
        print(f"[!] Unknown source: {source}")
        return
    
    if crawler.results:
        crawler.save_results('crawled_gaushalas.json')
    else:
        print("[!] No data found.")

if __name__ == "__main__":
    main()
